import { agentLoop, agentLoopContinue, } from "@mariozechner/pi-ai";
/**
 * Transport that calls LLM providers directly.
 * Optionally routes calls through a CORS proxy if configured.
 */
export class ProviderTransport {
    options;
    constructor(options = {}) {
        this.options = options;
    }
    async getModelAndKey(cfg) {
        let apiKey;
        if (this.options.getApiKey) {
            apiKey = await this.options.getApiKey(cfg.model.provider);
        }
        if (!apiKey) {
            throw new Error(`No API key found for provider: ${cfg.model.provider}`);
        }
        let model = cfg.model;
        if (this.options.corsProxyUrl && cfg.model.baseUrl) {
            model = {
                ...cfg.model,
                baseUrl: `${this.options.corsProxyUrl}/?url=${encodeURIComponent(cfg.model.baseUrl)}`,
            };
        }
        return { model, apiKey };
    }
    buildContext(messages, cfg) {
        return {
            systemPrompt: cfg.systemPrompt,
            messages,
            tools: cfg.tools,
        };
    }
    buildLoopConfig(model, apiKey, cfg) {
        return {
            model,
            reasoning: cfg.reasoning,
            apiKey,
            getQueuedMessages: cfg.getQueuedMessages,
        };
    }
    async *run(messages, userMessage, cfg, signal) {
        const { model, apiKey } = await this.getModelAndKey(cfg);
        const context = this.buildContext(messages, cfg);
        const pc = this.buildLoopConfig(model, apiKey, cfg);
        for await (const ev of agentLoop(userMessage, context, pc, signal)) {
            yield ev;
        }
    }
    async *continue(messages, cfg, signal) {
        const { model, apiKey } = await this.getModelAndKey(cfg);
        const context = this.buildContext(messages, cfg);
        const pc = this.buildLoopConfig(model, apiKey, cfg);
        for await (const ev of agentLoopContinue(context, pc, signal)) {
            yield ev;
        }
    }
}
//# sourceMappingURL=ProviderTransport.js.map